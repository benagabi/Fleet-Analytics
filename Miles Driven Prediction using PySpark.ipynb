{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################\n",
    "#                                          Data Fetching \n",
    "#################################################################################################################################################\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SQLContext\n",
    "from pyspark import HiveContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"Future Data Prep\")\n",
    "\n",
    "# dev\n",
    "conf.setMaster(\"spark://00.0.00.00:0000\")\n",
    "conf.set(\"spark.cassandrconnection.host\", \"00.0.00.00\")\n",
    "\n",
    "#sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "ssc = SQLContext(sc)\n",
    "\n",
    "# asset_info data\n",
    "\n",
    "asset_info_tbl = hc.read.format(\"org.apache.spark.sql.cassandra\").option(\"table\", \"cs_fuel_asset_cluster\").option(\"keyspace\", \"advanced_analytics\").load().filter(\" corp_cd='FA'\")\n",
    "\n",
    "asset_info_tbl = hc.read.format(\"filodb.spark\").option(\"dataset\", \"cs_fuel_asset_cluster\").option(\"database\", \"advanced_analytics_fdb\").load().filter(\" corp_cd='FA'\")\n",
    "#cli_no in ('7325','3056','3921') and\n",
    "\n",
    "asset_info_tbl.registerTempTable(\"asset_info\")\n",
    "\n",
    "eia_tbl = hc.read.format(\"filodb.spark\").option(\"dataset\", \"fuel_eia_series\").option(\"database\", \"advanced_analytics_fdb\").load()\n",
    "eia_tbl.registerTempTable(\"eia\")\n",
    "eia_info = hc.sql(\"select region, fuel_type, fuel_cost_amt/100 as fuel_cost_amt,period_name from eia\")\n",
    "\n",
    "'''asset_df = hc.sql(\"select corp_cd, cli_no,unit_no, spin_asset_id, invy_stat_cd,tank_capacity_imp,vndr_veh_class_cd_index, \\\n",
    "body_style_imp_index,no_of_cyl_imp,engine_cd_imp,title_inhouse_ind_index,model_cd_index, \\\n",
    "acq_purch_typ_cd_imp_index,cap_cost,drive_imp_index,st_prov_nm, \\\n",
    "drvd_model_index,drvd_make_index,engine_cd_imp_index,gvw_imp, \\\n",
    "mos_in_svc,no_doors_imp_index,series_index, \\\n",
    "prod_class_cd,tire_imp_index,age_of_vehicle \\\n",
    ",mfg_cd_imp_index,clust_no from asset_info\")'''\n",
    " \n",
    "asset_df= asset_info_tbl\n",
    "##############################################################################################################################################\n",
    "#                                                Missing Imputation\n",
    "##############################################################################################################################################\n",
    "asset_df = asset_df.na.fill('212121')\n",
    "\n",
    "#############################################################################################################################################\n",
    "###   \t\t\t\t\t\t\t\t\tMiles Prediction ,MPG and Product_Quantity\n",
    "#############################################################################################################################################\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import SQLContext\n",
    "from pyspark import HiveContext\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import col, udf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "hc = HiveContext(sc)\n",
    "ssc = SQLContext(sc)\n",
    "\n",
    "mile_tbl = hc.read.format(\"filodb.spark\").option(\"dataset\", \"estimates\").option(\"database\", \"advanced_analytics_fdb\").load().filter(\"corp_cd='FA'   and  period_name in (201701,201702,201703,201704,201705,201706,201707,201708,201709,201710,201710,201711,201712) and  estimate_type=2\")\n",
    "#and cli_no in ('7325','3056','17595','308261')\n",
    "\n",
    "mile_tbl.registerTempTable(\"mile\")\n",
    "\n",
    "asset_info_tbl = hc.read.format(\"filodb.spark\").option(\"dataset\", \"asset_info\").option(\"database\", \"core_fdb\").load().filter(\"curr_ast_cli_flag='Y'\").filter(\"corp_cd='FA' \")\n",
    "\n",
    "#and cli_no in ('7325','3056','17595','308261')\n",
    "asset_info_tbl.registerTempTable(\"asset_info\")\n",
    "\n",
    "miles_info = hc.sql(\"select mile.corp_cd, mile.cli_no,mile.inv_smry_dt,mile.spin_asset_id, mile.estimate_type,mile.period_name, mile.lou_est_value,mile.monthly_est_value,asset.city_loc_cd from mile mile inner join asset_info asset on asset.corp_cd=mile.corp_cd and asset.cli_no=mile.cli_no and asset.spin_asset_id=mile.spin_asset_id\")\n",
    "#miles_info.registerTempTable(\"mile_info\")\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "#\t\t\t\t\t\t\t\t\t\tColumn Creation\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "\n",
    "miles_info = miles_info.withColumn(\"period_name_f\",miles_info[\"period_name\"].cast(DoubleType()).alias(\"period_name\"))\n",
    "#asset_fuel_2016 = asset_fuel_2016.drop('period_name')\n",
    "\n",
    "miles_info= miles_info.withColumn('month_s', miles_info.period_name_f.substr(5,6))\n",
    "miles_info= miles_info.withColumn('month_num', miles_info.month_s.cast(DoubleType()))\n",
    "\n",
    "miles_info= miles_info.withColumn('year_s', miles_info.period_name_f.substr(1,4))\n",
    "miles_info= miles_info.withColumn('year_num', miles_info.month_s.cast(DoubleType()))\n",
    "\n",
    "miles_info= miles_info.withColumn('client_no', miles_info.cli_no.cast(DoubleType()))\n",
    "\n",
    "miles_info= miles_info.withColumn('city_cd', miles_info.city_loc_cd.cast(DoubleType()))\n",
    "\n",
    "miles_info.registerTempTable(\"mile_info\")\n",
    "\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "#\t\t\t\t\t\t\t\tcreating the vector for all predictors\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "\n",
    "#cli_no: string, inv_smry_dt: timestamp, spin_asset_id: int, estimate_type: int, period_name: int, lou_est_value: double, #monthly_est_value: double, city_loc_cd: string]\n",
    "\n",
    "miles_tl = miles_info.select('client_no','inv_smry_dt','spin_asset_id','period_name','month_num', \\\n",
    "'monthly_est_value','city_cd')\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, PolynomialExpansion, VectorIndexer\n",
    "\n",
    "feat_list = ['client_no','spin_asset_id','month_num','city_cd']\n",
    "\n",
    "assembler = VectorAssembler( inputCols = feat_list, outputCol = \"features\")\n",
    "\n",
    "#miles_data = assembler.transform( miles_tl)\n",
    "\n",
    "#miles_data = miles_data.withColumn( \"label\", miles_data.monthly_est_value.cast( 'double' ) )\n",
    "\n",
    "#miles_data.select( \"features\", \"label\" ).show( 5 )\n",
    "\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "#\t\t\t\t\t\t\t\tFeature Engg. \n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "\n",
    "from pyspark.sql.functions import lag, col, lead\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "########################## Average of Miles in last 3 months\n",
    "\n",
    "w = Window().partitionBy(\"cli_no\",\"spin_asset_id\").orderBy(\"inv_smry_dt\")\n",
    "\n",
    "miles_info = miles_info.withColumn(\"avg_miles_last_3months\",\n",
    "                                       ( lag(\"monthly_est_value\", 1).over(w)\n",
    "\t\t\t\t\t\t\t\t\t   + lag(\"monthly_est_value\", 2).over(w)\n",
    "\t\t\t\t\t\t\t\t\t\t+ lag(\"monthly_est_value\", 3).over(w)) /3  )\n",
    "\t\t\t\t\t\t\t\t\t   \n",
    "\t\t\t\t\t\t\t\t\t   \n",
    "\t\t\t\t\t\t\t\t\t   \n",
    "#w = Window().partitionBy(\"Store\").orderBy(\"Date\")\t\t\t\t\t\t\t\t\t   \n",
    "###################Average miles per last 6 months\n",
    "w = Window().partitionBy(\"cli_no\",\"spin_asset_id\").orderBy(\"inv_smry_dt\")\n",
    "miles_info = miles_info.withColumn(\"avg_miles_last_6months\",\n",
    "                                       ( lag(\"monthly_est_value\", 1).over(w)\n",
    "                                        + lag(\"monthly_est_value\", 2).over(w)\n",
    "                                        + lag(\"monthly_est_value\", 3).over(w)\n",
    "                                        + lag(\"monthly_est_value\", 4).over(w)\n",
    "                                        + lag(\"monthly_est_value\", 5).over(w)\n",
    "\t\t\t\t\t\t\t\t\t\t+ lag(\"monthly_est_value\", 6).over(w))/6)\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "###################Average miles per last 2 months\n",
    "\n",
    "w = Window().partitionBy(\"cli_no\",\"spin_asset_id\").orderBy(\"inv_smry_dt\")\t\t\t\t\t\t\t\t\n",
    "miles_info = miles_info.withColumn(\"avg_miles_last_2months\",\n",
    "                                       ( lag(\"monthly_est_value\", 1).over(w)\n",
    "                                        + lag(\"monthly_est_value\", 2).over(w))/2)\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "###################Average miles \t\t\t\t\t\t\t\t\t\n",
    "avg_miles_cli_spinid_by_month = miles_info.groupBy( \"cli_no\",\"spin_asset_id\" ).avg( \"monthly_est_value\" )\n",
    "\n",
    "#avg_miles_cli_spinid_by_month.show( 5 )\n",
    "\n",
    "miles_new_df = miles_info.join( avg_miles_cli_spinid_by_month, (miles_info.cli_no==avg_miles_cli_spinid_by_month.cli_no) & (miles_info.spin_asset_id==avg_miles_cli_spinid_by_month.spin_asset_id), \"inner\" ).drop(avg_miles_cli_spinid_by_month.spin_asset_id).drop(avg_miles_cli_spinid_by_month.cli_no)\n",
    "\n",
    "miles_new_df = miles_new_df.withColumnRenamed('AVG(monthly_est_value)', \"avg_monthly_value\")\t\t\t\t\t\t\t\t\t   \n",
    "\t\t\t\t\t\t\t\t\t   \n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "#\t\t\t\t\t\t\t\t   Variable Selection\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "\t\t\t\t\t\t\t\t   \n",
    "\t\t\t\t\t\t\t\t\t   \n",
    "miles_new_df = miles_new_df.select('client_no','spin_asset_id','city_cd','avg_miles_last_3months','avg_miles_last_6months','avg_miles_last_2months',\"avg_monthly_value\",\"month_num\",\"monthly_est_value\")\n",
    " \n",
    "miles_new_df=miles_new_df.fillna(0) \n",
    "      \n",
    "feat_list = feat_list + ['avg_miles_last_3months','avg_miles_last_6months','avg_miles_last_2months',\"avg_monthly_value\"]\n",
    "\n",
    "assembler = VectorAssembler( inputCols = feat_list, outputCol = \"features\")\n",
    "miles_new_df = assembler.transform( miles_new_df )\n",
    "miles_new_df = miles_new_df.withColumn( \"label\", miles_new_df.monthly_est_value.cast( 'double' ) )\n",
    "#miles_new_df.select( \"features\", \"label\" ).show( 5 )\n",
    "\n",
    "seed = 42\n",
    "\n",
    "train_df, test_df = miles_new_df.randomSplit( [0.7, 0.3], seed = seed )\n",
    "\n",
    "linreg = LinearRegression(maxIter=500, regParam=0.0)\n",
    "lm = linreg.fit( train_df )\n",
    "\n",
    "lm.intercept\n",
    "\n",
    "lm.coefficients\n",
    "\n",
    "y_pred = lm.transform( test_df )\n",
    "#y_pred.select( 'features', 'label', 'prediction' ).show( 5 )\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                            predictionCol=\"prediction\",\n",
    "                            metricName=\"r2\" )\n",
    "\n",
    "lm_r2 = r2_evaluator.evaluate( y_pred )\n",
    "lm_r2\n",
    "\n",
    "'''rmse_evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                            predictionCol=\"prediction\",\n",
    "                            metricName=\"rmse\" )\n",
    "\n",
    "lm_rmse = rmse_evaluator.evaluate(y_pred)\n",
    "lm_rmse'''\n",
    "                                                   \n",
    "##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#   \t\t\t\t\t\t\t\tFuture Data Prep\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++       \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import itertools\n",
    "\n",
    "'''def cartesian(df1, df2):\n",
    "    rows = itertools.product(df1.iterrows(), df2.iterrows())\n",
    "    df = pd.DataFrame(left.append(right) for (_, left), (_, right) in rows)\n",
    "    return df.reset_index(drop=True)'''\n",
    "\n",
    "future_id = hc.sql(\"select distinct  client_no,spin_asset_id,city_cd from mile_info\")\n",
    "\n",
    "futur = future_id.toPandas()\n",
    "\n",
    "import pandas as pd\n",
    "month_date = '2018-01-01'\n",
    "date_data = pd.DataFrame(pd.date_range(month_date, periods=12, freq='M'),columns=[\"date\"])\n",
    "date_data['year'] = pd.DatetimeIndex(date_data['date']).year\n",
    "date_data['month_num'] = pd.DatetimeIndex(date_data['date']).month\n",
    "date_data[\"period_name\"] = date_data[\"year\"].map(str) + date_data.month_num.map(\"{:02}\".format)\n",
    "\n",
    "futur['key'] = 0\n",
    "date_data['key'] = 0\n",
    "\n",
    "\n",
    "future_date = futur.merge(date_data, how='outer')\n",
    "\n",
    "future_data =  sqlContext.createDataFrame(future_date)\n",
    "\n",
    "future_data = future_data.join(miles_new_df['client_no','month_num','spin_asset_id','avg_monthly_value','avg_miles_last_2months','avg_miles_last_3months','avg_miles_last_6months'],(future_data.client_no==miles_new_df.client_no) & (future_data.spin_asset_id==miles_new_df.spin_asset_id) & (future_data.month_num==miles_new_df.month_num) ).drop(miles_new_df.spin_asset_id).drop(miles_new_df.client_no).drop(miles_new_df.month_num)\n",
    "\n",
    "feat_list1 = ['client_no','spin_asset_id','month_num','city_cd','avg_miles_last_3months','avg_miles_last_6months','avg_miles_last_2months',\"avg_monthly_value\"]\n",
    "\n",
    "assembler = VectorAssembler( inputCols = feat_list1, outputCol = \"features\")\n",
    "\n",
    "miles_fut_data = assembler.transform( future_data)\n",
    "\n",
    "miles_predicted = lm.transform(miles_fut_data)\n",
    "#miles_predicted\n",
    "\n",
    "miles_predicted = miles_predicted.withColumnRenamed('prediction', \"monthly_est_value\")\n",
    "miles_predicted = miles_predicted.withColumnRenamed('client_no', \"cli_no\")\n",
    "\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "#\t\t\t\t\t\t\t\t\t\t\t\tMPG Misssing Value Imputation\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "\n",
    "epa_tbl = hc.read.format(\"org.apache.spark.sql.cassandra\").option(\"table\", \n",
    "                                                                  \"fuel_epa_mpg\").option(\"keyspace\", \"advanced_analytics\").load()\n",
    "epa_tbl.registerTempTable(\"epa\")\n",
    "\n",
    "\n",
    "#epa_info = hc.sql(\"select epa.corp_cd, epa.cli_no, epa.spin_asset_id, epa.citympg, epa.hwympg, epa.combmpg from epa where  citympg is not null\")\n",
    "\n",
    "asset_info_tbl = hc.read.format(\"filodb.spark\").option(\"dataset\", \n",
    "                                                       \"asset_info\").option(\"database\", \"core_fdb\").load().filter(\"curr_ast_cli_flag='Y'\").filter(\"corp_cd='FA' \")\n",
    "\n",
    "#and cli_no in ('7325','3056','17595','308261')\n",
    "asset_info_tbl.registerTempTable(\"asset_info\")\n",
    "\n",
    "mpg_imputed_all = hc.sql(\"select asset.corp_cd \\\n",
    ",asset.cli_no \\\n",
    ",asset.spin_asset_id \\\n",
    ",epa.citympg \\\n",
    ",epa.combmpg \\\n",
    ",epa.hwympg \\\n",
    ",asset.prod_class_cd \\\n",
    ",case \\\n",
    "when epa.combmpg not in (0,NULL,'Infinity')  then epa.combmpg \\\n",
    "when epa.combmpg in (0,NULL,'Infinity') and epa_avg.combavg_m not in (0,NULL,'Infinity')   then epa_avg.combavg_m \\\n",
    "when epa.combmpg in (0,NULL,'Infinity') and epa_avg.combavg_m in (0,NULL,'Infinity') and epa_prd_reg_avg.combavg_m not in (0,NULL,'Infinity')  then epa_prd_reg_avg.combavg_m  \\\n",
    "when epa.combmpg in (0,NULL,'Infinity') and epa_avg.combavg_m in (0,NULL,'Infinity') and epa_prd_reg_avg.combavg_m  in (0,NULL,'Infinity') and epa_overll_avg.combavg_m not in (0,NULL,'Infinity') then epa_overll_avg.combavg_m \\\n",
    "end as mpg \\\n",
    ",epa_avg.combavg_m  \\\n",
    ",epa_prd_reg_avg.combavg_m  \\\n",
    ",epa_overll_avg.combavg_m  \\\n",
    "from epa epa  \\\n",
    "right join asset_info asset \\\n",
    "on asset.corp_cd=epa.corp_cd and asset.cli_no=epa.cli_no and asset.spin_asset_id=epa.spin_asset_id \\\n",
    "left outer join  \\\n",
    "(select prod_class_cd,drvd_model,engine_cd,geo_region_cd,avg(combmpg) as combavg_m \\\n",
    "from epa epa \\\n",
    "inner join asset_info asset \\\n",
    "on asset.corp_cd=epa.corp_cd and asset.cli_no=epa.cli_no and asset.spin_asset_id=epa.spin_asset_id \\\n",
    "where combmpg not in (0,NULL,'Infinity') \\\n",
    "group by prod_class_cd,drvd_model,engine_cd,geo_region_cd) epa_avg \\\n",
    "on \\\n",
    "epa_avg.prod_class_cd = asset.prod_class_cd \\\n",
    "and epa_avg.drvd_model = asset.drvd_model \\\n",
    "and epa_avg.engine_cd = asset.engine_cd \\\n",
    "and epa_avg.geo_region_cd = asset.geo_region_cd \\\n",
    "left outer join \\\n",
    "(select prod_class_cd,geo_region_cd,avg(combmpg) as combavg_m \\\n",
    "from fdb_adv_ana_fuel_epa_mpg epa \\\n",
    "inner join fdb_core_asset_info asset \\\n",
    "on asset.corp_cd=epa.corp_cd and asset.cli_no=epa.cli_no and asset.spin_asset_id=epa.spin_asset_id \\\n",
    "where combmpg not in (0,NULL,'Infinity') \\\n",
    "group by prod_class_cd,geo_region_cd) epa_prd_reg_avg \\\n",
    "on \\\n",
    "epa_prd_reg_avg.prod_class_cd = asset.prod_class_cd \\\n",
    "and epa_prd_reg_avg.geo_region_cd = asset.geo_region_cd \\\n",
    "left outer join \\\n",
    "(select prod_class_cd,avg(combmpg) as combavg_m \\\n",
    "from fdb_adv_ana_fuel_epa_mpg epa \\\n",
    "inner join fdb_core_asset_info asset \\\n",
    "on asset.corp_cd=epa.corp_cd and asset.cli_no=epa.cli_no and asset.spin_asset_id=epa.spin_asset_id \\\n",
    "where combmpg not in (0,NULL,'Infinity') \\\n",
    "group by prod_class_cd) epa_overll_avg \\\n",
    "on \\\n",
    "epa_overll_avg.prod_class_cd= asset.prod_class_cd \\\n",
    "where asset.corp_cd = 'FA' \")\n",
    "\n",
    "#and asset.cli_no in ('7325','3056','17595','308261')\n",
    "\n",
    "#from functools import reduce  # For Python 3.x\n",
    "#from pyspark.sql import DataFrame\n",
    "\n",
    "#def unionAll(*dfs):\n",
    "    #return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "#mpg_imputed_all =unionAll(mpg_imputed,mpg_org)\n",
    " \n",
    "#imputed_mpg.registerTempTable(\"imputed_mpg\")\n",
    "#chk= hc.sql(\"select corp_cd,cli_no,spin_asset_id,prod_class_cd,drvd_model,engine_cd,geo_region_cd,combmpg,combavg_m \\\n",
    "#from imputed_mpg where combavg_m in (0,NULL,'Infinity')\")\n",
    "\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "#                                      Product Quantity Prep\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
    "\n",
    "miles_mpg = miles_predicted['cli_no',\n",
    "                            'spin_asset_id','period_name',\n",
    "                            'monthly_est_value'].join(mpg_imputed_all['spin_asset_id',\n",
    "                                                                      'prod_class_cd', 'mpg'],(miles_predicted.spin_asset_id == mpg_imputed_all.spin_asset_id),\"left_outer\").drop(mpg_imputed_all.spin_asset_id)\n",
    "\n",
    "#sqlContext.registerDataFrameAsTable(miles_mpg, \"miles_mpg_d\")\n",
    "#miles_mpg.registerTempTable(\"miles_mpg_d\")\n",
    "#miles_mpg = hc.sql(\"select a.corp_cd,a.cli_no,a.spin_asset_id,a.prod_class_cd,a.drvd_model,a.engine_cd,a.geo_region_cd,b.combavg_m as #combmpg \\\n",
    "#from miles_mpg_d a left join epa_overll_avg b  where  a.prod_class_cd=b.prod_class_cd  and  a.combmpg in (0,NULL,'Infinity')\")\n",
    "\n",
    "miles_mpg = miles_mpg.withColumn('prod_qnty', miles_mpg.monthly_est_value/miles_mpg.mpg)\n",
    "\n",
    "############################################################Region Creation\n",
    "# add region\n",
    "PADD = {\n",
    "    'East Coast': ['ME', 'NH', 'MA', 'RI', 'CT', 'NY', 'NJ', 'DE', 'MD', 'VA', 'NC', 'SC', 'GA', 'FL', 'WV', 'PA', 'VT',\n",
    "                   'DC'],\n",
    "    'Gulf Coast': ['NM', 'TX', 'AR', 'LA', 'MS', 'AL'],\n",
    "    'Rocky Mountain': ['ID', 'MT', 'WY', 'CO', 'UT'],\n",
    "    'Midwest': ['IL', 'IN', 'OH', 'MI', 'WI', 'MO', 'NE', 'MN', 'KS', 'ND', 'SD', 'IA', 'OK', 'TN', 'KY'],\n",
    "    'West Coast': ['CA', 'OR', 'WA', 'AK', 'NV', 'AZ', 'HI']}\n",
    "\n",
    "def get_region(s):\n",
    "    for region, states in PADD.iteritems():\n",
    "        for state in states:\n",
    "            if state in s:\n",
    "                return region\n",
    "\n",
    "udfgetregion = udf(get_region, StringType())\n",
    "\n",
    "# create column for abbr version of state\n",
    "STATE = {'ME': 'MAINE',\n",
    "         'NH': 'NEW HAMPSHIRE',\n",
    "         'MA': 'MASSACHUSETTS',\n",
    "         'RI': 'RHODE ISLAND',\n",
    "         'CT': 'CONNECTICUT',\n",
    "         'NY': 'NEW YORK',\n",
    "         'NJ': 'NEW JERSEY',\n",
    "         'DE': 'DELAWARE',\n",
    "         'MD': 'MARYLAND',\n",
    "         'VA': 'VIRGINIA',\n",
    "         'NC': 'NORTH CAROLINA',\n",
    "         'SC': 'SOUTH CAROLINA',\n",
    "         'GA': 'GEORGIA',\n",
    "         'FL': 'FLORIDA',\n",
    "         'WV': 'WEST VIRGINIA',\n",
    "         'PA': 'PENNSYLVANIA',\n",
    "         'VT': 'VERMONT',\n",
    "         'DC': 'distRICT OF COLUMBIA',\n",
    "         'NM': 'NEW MEXICO',\n",
    "         'TX': 'TEXAS',\n",
    "         'AR': 'ARKANSAS',\n",
    "         'LA': 'LOUISIANA',\n",
    "         'MS': 'MISSISSIPPI',\n",
    "         'AL': 'ALABAMA',\n",
    "         'ID': 'IDAHO',\n",
    "         'MT': 'MONTANA',\n",
    "         'WY': 'WYOMING',\n",
    "         'CO': 'COLORADO',\n",
    "         'UT': 'UTAH',\n",
    "         'IL': 'ILLINOIS',\n",
    "         'IN': 'INDIANA',\n",
    "         'OH': 'OHIO',\n",
    "         'MI': 'MICHIGAN',\n",
    "         'WI': 'WISCONSIN',\n",
    "         'MO': 'MISSOURI',\n",
    "         'NE': 'NEBRASKA',\n",
    "         'MN': 'MINNESOTA',\n",
    "         'KS': 'KANSAS',\n",
    "         'ND': 'NORTH DAKOTA',\n",
    "         'SD': 'SOUTH DAKOTA',\n",
    "         'IA': 'IOWA',\n",
    "         'OK': 'OKLAHOMA',\n",
    "         'TN': 'TENNESSEE',\n",
    "         'KY': 'KENTUCKY',\n",
    "         'CA': 'CALIFORNIA',\n",
    "         'OR': 'OREGON',\n",
    "         'WA': 'WASHINGTON',\n",
    "         'AK': 'ALASKA',\n",
    "         'NV': 'NEVADA',\n",
    "         'AZ': 'ARIZONA',\n",
    "         'HI': 'HAWAII'}\n",
    "\n",
    "def get_state_abbr(s):\n",
    "    for state_abbr, statefull in STATE.items():\n",
    "        if s == statefull:\n",
    "            return state_abbr\n",
    "\n",
    "udfgetabr = udf(get_state_abbr, StringType())\n",
    "\n",
    "asset_df = asset_df.withColumn(\"state\", udfgetabr(asset_df[\"st_prov_nm\"]))\n",
    "\n",
    "asset_df = asset_df.withColumn(\"region\",udfgetregion(asset_df['state']))\n",
    "############################################################################################################################################\n",
    "#    Joining Miles,MPG  and EIA Price  Data\n",
    "############################################################################################################################################\n",
    "\n",
    "asset_mpg_df = asset_df.join(miles_mpg['spin_asset_id', 'period_name','monthly_est_value','prod_qnty','mpg'], \n",
    "                             (asset_df.spin_asset_id == asset_df.spin_asset_id),\n",
    "                             \"inner\").drop(miles_mpg.spin_asset_id)\n",
    "\n",
    "asset_mpg_df = asset_mpg_df.join(eia_info['region','period_name','fuel_cost_amt'], \n",
    "                                 (eia_info.region == asset_mpg_df.region) \n",
    "                                 & (eia_info.period_name == asset_mpg_df.period_name),\n",
    "                                 \"left_outer\").drop(asset_mpg_df.region).drop(eia_info.period_name)\n",
    "\n",
    "####################################################################################################################################################   creating a Column with month name\n",
    "#########################################################################################################################################\n",
    "asset_mpg_df = asset_mpg_df.withColumn(\"period_name_f\",asset_mpg_df[\"period_name\"].cast(DoubleType()).alias(\"period_name\"))\n",
    "#asset_fuel_2016 = asset_fuel_2016.drop('period_name')\n",
    "\n",
    "asset_mpg_df= asset_mpg_df.withColumn('month_s', asset_mpg_df.period_name_f.substr(5,6))\n",
    "asset_mpg_df= asset_mpg_df.withColumn('month_num', asset_mpg_df.month_s.cast(DoubleType()))\n",
    "\n",
    "#asset_mpg_df= asset_mpg_df.withColumn('no_of_doors_imp', asset_mpg_df.no_doors_imp.cast(DoubleType()))\n",
    "\n",
    "asset_mpg_df = asset_mpg_df.withColumn(\"month\", when((col('month_s') == \"01.0\"), \"Jan\") \\\n",
    "                                       .when((col('month_s') == \"02.0\") , \"Feb\") \\\n",
    "                                       .when((col('month_s') == \"03.0\") , \"Mar\") \\\n",
    "                                       .when((col('month_s') == \"04.0\"), \"Apr\") \\\n",
    "                                       .when((col('month_s') == \"05.0\"), \"May\") \\\n",
    "                                       .when((col('month_s') == \"06.0\"), \"Jun\") \\\n",
    "                                       .when((col('month_s') == \"07.0\"), \"Jul\") \\\n",
    "                                       .when((col('month_s') == \"08.0\"), \"Aug\") \\\n",
    "                                       .when((col('month_s') == \"09.0\"), \"Sep\") \\\n",
    "                                       .when((col('month_s') == \"10.0\"), \"Oct\") \\\n",
    "                                       .when((col('month_s') == \"11.0\"), \"Nov\") \\\n",
    "                                       .when((col('month_s') == \"12.0\"), \"Dec\") \\\n",
    "                                       .otherwise(col(\"month_s\")))\n",
    "\n",
    "############################################################################################################################################\n",
    "#  Data  Uploading \n",
    "\n",
    "#feature_dpython = feature_data.toPandas()\n",
    "#feature_dpython.to_csv(\"/home/dev/python_models/feature_dpython.csv\")\n",
    " \n",
    "#feature_data.registerTempTable(\"feature_data\")\n",
    "run_date = datetime.datetime.now()\n",
    "asset_mpg_df = asset_mpg_df.withColumn('run_date', lit(run_date))\n",
    "\n",
    "asset_mpg_df = asset_mpg_df.sort(\"corp_cd\",\"cli_no\")\n",
    "\n",
    "asset_mpg_df.write.format(\"org.apache.spark.sql.cassandra\").mode('append').options(keyspace=\"advanced_analytics\",table=\"cs_fuel_asset_future_data\").save()\n",
    "ssc.clearCache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
